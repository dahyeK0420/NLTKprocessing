{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Semi-Structured Data and Export as XML\n",
    "#### Author Name: Dahye Kim\n",
    "\n",
    "Date: 02/09/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.9 and Jupyter notebook\n",
    "\n",
    "Libraries used:\n",
    "* re (for regular expression, included in Anaconda Python 3) \n",
    "* os (for numpy array, included in Anaconda Python 3) \n",
    "* langid (for numpy array, included in Anaconda Python 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# loop through all the documents in working directory \n",
    "\n",
    "import re \n",
    "# regular expression \n",
    "\n",
    "import langid\n",
    "# classify the language of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parse Twitters\n",
    "\n",
    "To parse the text files containing all the tweets, I used os.listdir() to retrieve the file names in the working directory. Then I looped through the names of the files in the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = os.listdir('tweetsJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Parse as Text File using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "\n",
    "for textFileName in fileNames: \n",
    "    data = open('tweetsJSON/'+textFileName)\n",
    "    # opening the file in the working directory \n",
    "\n",
    "    for i in data.read().split('},{'):\n",
    "    # since the id of the tweet users, created date, and the tweets are seperated by {}\n",
    "    # I split the chunk with '},{'\n",
    "\n",
    "        if not re.findall(r'\"id\":\"(\\d{19})\"', i) and \\\n",
    "        not re.findall(r'\"text\":\"(.+?)\"', i) and \\\n",
    "        not re.findall(r'\"created_at\":\"(.+?)\"', i):    \n",
    "            continue \n",
    "            # every single valid tweet requires the text, the id, and the created date\n",
    "            # if these three factors are not present in the line, the line is omitted\n",
    "\n",
    "        else: \n",
    "            singleTweet = {}\n",
    "            # each tweet are collected into its own dictionary \n",
    "            \n",
    "            singleTweet['text']=re.search(r'\"text\":\"(.+?)\"', i).group(1)\n",
    "            # the tweet is assigned with the key 'tweet' into the dictionary singleTweet\n",
    "            singleTweet['id']=re.search(r'\"id\":\"(\\d{19})\"', i).group(1)\n",
    "            # the id of the tweet user is assigned with the key 'id' into the dictionary \n",
    "            singleTweet['createdAt']=re.search(r'\"created_at\":\"(.+?)\"', i).group(1)\n",
    "            # the created date of the tweet is assigned with the key 'createdAt' into the dictionary \n",
    "\n",
    "            tweets.append(singleTweet)\n",
    "            # each singleTweet dictionary composed of a single piece of tweet with its relevant information \n",
    "            # is appended into the list tweets \n",
    "            \n",
    "    data.close()\n",
    "\n",
    "tweets = [dict(t) for t in {tuple(d.items()) for d in tweets}]\n",
    "# any duplicate tweets are removed using set() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Parse as ``json`` Using ``json`` Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "allTweets = list()\n",
    "for index in range(len(fileNames)):\n",
    "    handle = open('tweetsJSON/'+fileNames[index])\n",
    "    textFormat = handle.read()\n",
    "    allTweets.append(json.loads(textFormat)['data'])\n",
    "    handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "individualTweet= [singleTweet for dailyTweets in allTweets for singleTweet in dailyTweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unescape Backslash in Each Tweet \n",
    "\n",
    "For each tweet in parsed from the text files, a lot of them contains double backslashes. \n",
    "This hampers us from decoding and encoding the surrogate pairs or printing out the escape sequence. \n",
    "Therefore, before writing the tweet into xml, I used the following procedures: \n",
    "\n",
    "1. Remove double backslash at the end of the tweets if they have one \n",
    "2. Replace double backslash of the escape sequence with single backslash \n",
    "3. For the emojis, first retrieve the tweets which contains the emojis via calling regex\n",
    "4. After retrieving the tweets with emoji texts, unescape the backslashes \n",
    "5. Encode and decode the surrogate pairs \n",
    "6. Filter the tweets that are not in English language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTweets = []\n",
    "\n",
    "for i in range(len(tweets)): \n",
    "# looping through every tweet in the list tweets\n",
    "\n",
    "    if tweets[i]['text'].endswith('\\\\'): \n",
    "    # if the tweet ends with'\\', .decode('unicode_escape') function cannot be used \n",
    "        tweets[i]['text'] = tweets[i]['text'].replace('\\\\','')\n",
    "        # remove the backslash at the end of the tweet if the tweets do end with a backslash \n",
    "        \n",
    "    if re.findall(r'(\\\\n)',tweets[i]['text']): \n",
    "    # if the tweet contains escape sequence but with double backslash \n",
    "        tweets[i]['text'] = tweets[i]['text'].replace('\\\\n','\\n')\n",
    "        # replace it with single backslash \n",
    "        \n",
    "    if re.findall(r'(\\\\uD\\S{3})',tweets[i]['text']): \n",
    "    # retrieve the tweets with emojis by using '\\\\ud\\S{3}'\n",
    "        tweets[i]['text']=tweets[i]['text'].encode('utf-8').decode('unicode_escape').encode(\"utf-16\", \"surrogatepass\").decode(\"utf-16\")\n",
    "        # unescape backslashes and then encode & decode the surrogate pairs \n",
    "        \n",
    "    if langid.classify(tweets[i]['text'])[0]!='en': continue \n",
    "    # omit all the tweets that are not in English \n",
    "    # this includes the decoded emojis, which is recognised as latin by langid.classify()\n",
    "    \n",
    "    filteredTweets.append(tweets[i])\n",
    "    # append all the corrected tweets to filteredTweets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filteredTweets=sorted(filteredTweets, key = lambda x: x['createdAt'])\n",
    "# sort the filteredTweets list based on the created date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Writing in the XML File \n",
    "\n",
    "After filtering the tweets and unescaping sequences, we can finally write in the XML file. \n",
    "For the XML, we first need to decide the tags of the XML. \n",
    "\n",
    "The tags are as follow: \n",
    "1. ```<data>``` - containing all the tweets from the filteredTweets file \n",
    "2. ```<tweets>``` - this tag contains all the tweets created in one single day. Therefore the attribute of the tag is the created date of the tweets, which is ```date=```\n",
    "3. ```<tweet>``` - this tag contains individual tweet and the user ID of the tweet. The attribute of ```<tweet>``` tag is the ```id=```.\n",
    "\n",
    "Each tag should be closed up after inserting the relevant information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetXML = open('scrapedTweet.xml', 'w')\n",
    "# open an empty xml file to write in the tweets \n",
    "\n",
    "tweetXML.write('''<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<data>\\n''')\n",
    "# insert the XML version and the encoding method, followed by the starting tag of 'data'\n",
    "\n",
    "for index in range(len(filteredTweets)): \n",
    "    if index == 0: \n",
    "    # for the first tweet, writing in the date created directly \n",
    "    #followed by the actual tweet and the user ID \n",
    "        tweetXML.write('''<tweets date=\"'''+\\\n",
    "                       filteredTweets[index]['createdAt'].split('T')[0]+\\\n",
    "                       '''\">\\n<tweet id=\"'''+\\\n",
    "                       filteredTweets[index]['id']+\\\n",
    "                       '''\">'''\\\n",
    "                       +filteredTweets[index]['text']\\\n",
    "                       +'</tweet>\\n')\n",
    "        \n",
    "    elif filteredTweets[index]['createdAt'].split('T')[0] != filteredTweets[index-1]['createdAt'].split('T')[0]: \n",
    "    # if the created date of the former tweet and the tweet in current loop differs\n",
    "    # a new tag is created with a different created date \n",
    "        tweetXML.write('''</tweets>\\n<tweets date=\"'''+\\\n",
    "                       filteredTweets[index]['createdAt'].split('T')[0]+\\\n",
    "                       '''\">\\n<tweet id=\"'''+filteredTweets[index]['id']+'''\">'''+\\\n",
    "                       filteredTweets[index]['text']+'</tweet>\\n')  \n",
    "    else: \n",
    "        # if the created date of the former tweet written into the file and that of the tweet in current loop are identical \n",
    "        # we do not need to create a new tag with a new created date in it \n",
    "        tweetXML.write('''<tweet id=\"'''+filteredTweets[index]['id']+'''\">'''+filteredTweets[index]['text']+'</tweet>\\n')\n",
    "        \n",
    "tweetXML.write('''</tweets>\\n</data>''')\n",
    "# end the XML file by closing all the tags \n",
    "\n",
    "tweetXML.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography \n",
    "\n",
    "How to open multiple files in a directory. (2016, August 17). Stack Overflow. https://stackoverflow.com/questions/38991923/how-to-open-multiple-files-in-a-directory/38992988\n",
    "\n",
    "How to un-escape a backslash-escaped string? (2009, December 11). Stack Overflow. https://stackoverflow.com/questions/1885181/how-to-un-escape-a-backslash-escaped-string\n",
    "\n",
    "How to work with surrogate pairs in Python? (2016, July 1). Stack Overflow. https://stackoverflow.com/questions/38147259/how-to-work-with-surrogate-pairs-in-python\n",
    "\n",
    "Remove duplicate dict in list in Python. (2012, February 24). Stack Overflow. https://stackoverflow.com/questions/9427163/remove-duplicate-dict-in-list-in-python\n",
    "\n",
    "Unescaping escaped characters in a string using Python 3.2. (2012, February 18). Stack Overflow. https://stackoverflow.com/questions/9339630/unescaping-escaped-characters-in-a-string-using-python-3-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
