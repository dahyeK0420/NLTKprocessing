{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Text Data with NLTK\n",
    "#### Author Name: Dahye Kim\n",
    "\n",
    "Date: 02/09/2020\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.7.9 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* pandas (for reading excel and create data frame with the tweets, included in Anaconda Python 3) \n",
    "* nltk (for tokenisation, frequency calculation, building matrix for distFreq) \n",
    "* xlrd (for parsing excel file)\n",
    "* numpy (for wrangling the excel file to data frame) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division # chaining series of iterative lists\n",
    "from itertools import chain \n",
    "\n",
    "import pandas as pd # parsing Excel file and organise the parsed object into data frame \n",
    "import xlrd # reading excel file and parse each sheet in excel file \n",
    "import langid # identify the language used in the tweet \n",
    "import nltk # for text analysis - including retrieving document frequency, vocab frequency, extracing bigrams, etc. \n",
    "import numpy as np # for wrangling the data frame \n",
    "from nltk.stem import PorterStemmer # stemming tokens after necessary filtering steps \n",
    "from nltk.probability import FreqDist as fd # retrieving document frequency and vocab frequency\n",
    "from nltk.util import ngrams # retrieving all the possible bigram combinations \n",
    "from sklearn.feature_extraction.text import CountVectorizer as cv # create sparse matrix using count vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Parsing the excel file and create data frame for all the tweets\n",
    "\n",
    "To parse the excel file, I imported pandas and xlrd. After reading the excel file with ```ExcelFile()``` function from pandas. To create a data frame for each excel sheet, I used pandas and numpy for possible missing values or errors in the parsing process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.ExcelFile('semiStructuredTweets.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "# all the excel tweets parsed by .parse() function are appended into dfs\n",
    "\n",
    "for i in range(len(excel_data.sheet_names)):\n",
    "    df = excel_data.parse(i)\n",
    "    # a new data frame created by the .parse() function \n",
    "    \n",
    "    df.dropna(axis = 1, how = 'all', inplace = True)\n",
    "    df.dropna(axis = 0, how = 'all', inplace = True)\n",
    "    # drop all the empty rows and columns \n",
    "    \n",
    "    df.columns = df.iloc[0,:]\n",
    "    # the new data frame's column name is the first row of the data frame ('text', 'id', 'created_at')\n",
    "    \n",
    "    df.drop(df.index[0], axis = 0, inplace = True)\n",
    "    # the first row, after becoming the column name of each data frame, is dropped \n",
    "    \n",
    "    df.iloc[:,2]=df.iloc[:,2].apply(lambda x: x.split('T')[0])\n",
    "    # only keeping the date for each tweet \n",
    "    # the created-date of the tweet is in the third column of each data frame \n",
    "    \n",
    "    df.reset_index(inplace = True, drop=True)\n",
    "    # the row index is reset \n",
    "    \n",
    "    dfs.append(df)\n",
    "    # new data frame is appended into dfs list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looping through each data frame, I realised that some data frame did not have ```['text', 'id', 'created_at']``` as the column names. These data frames had the first piece of tweet in the data frame assigned as the column name. I corrected this by inserting the column name as a new row in the data frame using numpy. While looping through each data frame, I also called ```langid.classify()``` function for each tweet and identified if they are English tweets. All non-English tweets are dropped after the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dahyekim/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/Users/dahyekim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/dahyekim/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer as tokeniser \n",
    "tokenizer = tokeniser(r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\")\n",
    "tokens = dict()\n",
    "\n",
    "for i in range(len(excel_data.sheet_names)): \n",
    "# looping through each data frame in dfs \n",
    "\n",
    "    if dfs[i].columns[0] != 'text' or dfs[i].columns[1]!='id' or dfs[i].columns[2]!='created_at': \n",
    "    # the data frame whose column names are not ['text', 'id', 'created_at'], the first tweet should've been assigned \n",
    "    # as the column name.\n",
    "    \n",
    "        pd.DataFrame(np.insert(df.values, 0, values = dfs[i].columns, axis = 0))\n",
    "        # the column names is inserted into the first row of the data frame with np.insert()\n",
    "        dfs[i].columns = ['text', 'id', 'created_at']\n",
    "        # the column names of the data frame is newly assigned \n",
    "        \n",
    "    dfs[i]['lang'] = dfs[i]['text'].apply(lambda x: langid.classify(str(x))[0])\n",
    "    # identify the language of each piece of tweet with langid.classify(). A new column 'lang' is created\n",
    "    dfs[i] = dfs[i][dfs[i].lang == 'en']\n",
    "    # keeping the only the tweets whose language is English\n",
    "    dfs[i].drop(['lang'], inplace = True, axis = 1)\n",
    "    # drop the 'lang' column after using it for filtering purpose \n",
    "\n",
    "    dfs[i]['text']=dfs[i]['text'].apply(lambda x: str(x).lower())\n",
    "    # change all the tweets to lower-case letters \n",
    "    dfs[i]['tokens']=dfs[i]['text'].apply(lambda x: tokenizer.tokenize(x))\n",
    "    # create a new column called 'tokens', which contains all the tokens of respective tweets \n",
    "    tokens[dfs[i].iloc[0,2]] = list(chain.from_iterable(list(dfs[i]['tokens'])))\n",
    "    # create a key-value pair in the dictionary token, whose key is the created_date of tweets in each sheet\n",
    "    # the value of the key is the list of tokens from each tweet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Corpus Vocabularies\n",
    "\n",
    "For the corpus vocabularies, I first retrieve the 'bag of words' containing all the words that occurred in every single tweet. The list ```words``` contains all the words with repetition, and the set ``vocabs`` keeps only the unique vocab. After that I retrieved all the bigrams and unigrams for the corpus vocabulary list. Following is the step of procedures I have taken: \n",
    "\n",
    "* Retrieving bigrams \n",
    "    1. Use the CollocationFinder to locate all the possible bigrams in the bag of words \n",
    "    2. Filter all the bigrams which contains the words, whose length is less than 3\n",
    "    3. Use the PMI measure to retrieve the top 200 meaningful bigrams from the filtered list of bigrams \n",
    "    4. Join the bigrams to appropriate format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Vocab Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(chain.from_iterable(tokens.values()))\n",
    "# create a list of words composed of the tokens from all the tweets\n",
    "vocabs = set(words)\n",
    "# use set() function to retrieve unique words and remove repetitive words from the list words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "bigram_finder = nltk.collocations.BigramCollocationFinder.from_words(words)\n",
    "# use collocations to locate the bigrams from the bag of words \n",
    "\n",
    "bigram_finder.apply_word_filter(lambda w: len(w) < 3)\n",
    "# filter out the bigrams whose composed word's length is less than 3 \n",
    "\n",
    "vocabBigrams = bigram_finder.nbest(bigram_measures.pmi, 200)\n",
    "# use the PMI measure to retrieve 200 most meaningful bigrams \n",
    "vocabBigrams = [i[0]+'_'+i[1] for i in vocabBigrams]\n",
    "# apply 'xxx_xxx' format to all the bigrams retrieved "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible shortcomings to this approach: \n",
    "\n",
    "When retrieving the most meaningful bigrams from the bag of words, I filtered all the bigrams that contains the words, whose length is less than 3. This could filter out a lot of bigrams that could also be critical for analysing the tweets that are to do with specific topics. \n",
    "When creating a corpus vocabulary, it could be more reasonable to retrieve the most meaningful bigrams by available measures first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Vocab Unigrams  \n",
    "\n",
    "To retrieve the unigrams, I followed the steps as below: \n",
    "\n",
    "1. Use ```FreqDist()``` to retrieve the document frequency of each unique word in a tweet. \n",
    "2. Creating the list of context-independent stopwords, context-dependent stopwords, and the rare tokens based on the document frequency and the given context-independent stopwords. \n",
    "3. Filter the words in the list of context-independent stopwords. \n",
    "4. Filter the words in the list of context-dependent stopwords. \n",
    "5. Remove the tokens whose length is less than 3. \n",
    "4. Use ```PorterStemmer()``` to stem the rest of the tokens. \n",
    "5. Create a set of unique unigrams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 context-independent and -dependent stopwords  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set([word.rstrip() for word in open('stopwords_en.txt', 'r').readlines()])\n",
    "# loop through each line in 'stopwords_en.txt' to retrieve all the context-independent stopwords \n",
    "\n",
    "freqDist = fd(list(chain.from_iterable([set(value) for value in tokens.values()])))\n",
    "# get the document frequency of each token with freqDist function by chaining the list of tokens of each value in the \n",
    "# tokens dictionary \n",
    "\n",
    "frequentTokens = set([key for key, val in freqDist.items() if val >60])\n",
    "# the threshold of the context-dependent stopwords is 60. \n",
    "# The tokens whose document frequency is larger than 60 is categorised as context-dependent stopwords \n",
    "\n",
    "rareTokens = set([key for key, val in freqDist.items() if val <5])\n",
    "# the threshold of the rare tokens is 5. \n",
    "# The tokens whose document frequency is smaller than 5 is categorised as rare tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Extracting unigrams  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "# the unigrams are stemmed with porter stemmer \n",
    "\n",
    "unigrams = list(chain.from_iterable(tokens.values()))\n",
    "# create a list of words composed of the tokens from all the tweets\n",
    "\n",
    "unigrams =[stemmer.stem(w) \\\n",
    "           for w in unigrams \\\n",
    "           if w not in stopwords \\\n",
    "           if w not in frequentTokens \\\n",
    "           if len(w)>=3 \\\n",
    "           if w not in rareTokens]\n",
    "# remove all the context-independent stopwords from the list of tokens \n",
    "\n",
    "vocabUnigrams = set(unigrams)\n",
    "# extract unique words from the refined list of tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Writing the corpus vocabularies into a file  \n",
    "\n",
    "Each vocabulary in the corpus vocabulary file is assinged with a unique integer ID. This ID is utilised when creating sparce matrix of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleVocab = open('vocab.txt', 'w')\n",
    "\n",
    "corpusVocabs = list(vocabUnigrams) + list(vocabBigrams)\n",
    "# create a list of corpus vocabularies by adding the list of unigrams and bigrams together \n",
    "\n",
    "for index, item in enumerate(sorted(corpusVocabs)): \n",
    "    # sort the list of corpus vocabulary when writing into the file \n",
    "    sampleVocab.write('{}:{}'.format(item, index))\n",
    "    # assigning token id to each token \n",
    "    sampleVocab.write('\\n')\n",
    "\n",
    "sampleVocab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Unigrams  \n",
    "\n",
    "To extract unigrams from each document, I followed the steps for each 'bag of words' from each document as below: \n",
    "\n",
    "1. Filter the words in the list of context-independent stopwords. \n",
    "2. Filter the words in the list of context-dependent stopwords. \n",
    "3. Remove the tokens whose length is less than 3. \n",
    "4. Use ```PorterStemmer()``` to stem the rest of the tokens. \n",
    "5. Create a set of unique unigrams for each document.\n",
    "6. Use ```FreqDist().most_common()``` function to extract the top 100 most common unigrams and its frequency, followed by sorting the list based on the frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyUnigrams=dict()\n",
    "\n",
    "for date in tokens.keys(): \n",
    "    temp = [stemmer.stem(w) for w in tokens[date]\\\n",
    "           if w not in stopwords \\\n",
    "           if w not in frequentTokens \\\n",
    "           if len(w)>=3 \\\n",
    "           if w not in rareTokens]\n",
    "    dailyUnigrams[date]=fd(temp).most_common(100)\n",
    "    # generate 100 most frequent unigrams from each day of tweets and sort them based on the frequency\n",
    "    # remove duplicate bigram-frequency tuple from the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Writing the 100 most frequent unigrams of each day and their frequencies to a file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleVocab = open('100uni.txt', 'w')\n",
    "\n",
    "for key, value in dailyUnigrams.items(): \n",
    "    sampleVocab.write('{}:{}'.format(key, value))\n",
    "    sampleVocab.write('\\n')\n",
    "\n",
    "sampleVocab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Bigrams  \n",
    "\n",
    "To extract bigrams from each document, I followed the steps for each 'bag of words' from each document as below: \n",
    "\n",
    "1. Create all the possible combination of the unigram with the bag of words\n",
    "6. Use ```FreqDist().most_common()``` function to extract the top 100 most common unigrams and its frequency, followed by sorting the list based on the frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyBigrams=dict()\n",
    "\n",
    "for date in tokens: \n",
    "    fdbigram = fd(ngrams(tokens[date], n = 2))\n",
    "    \n",
    "    # with ngrams() we generate all the possible combination of bigrams with the bag of words \n",
    "    # then we use freqDist to calculate the frequency of each bigram combinations \n",
    "    \n",
    "    dailyBigrams[date] = sorted(list(set(fdbigram.most_common(100))), key = lambda x: x[1], reverse = True)\n",
    "    # each bigrams are presented as tuples\n",
    "    # the list of bigrams is then sorted based on the frequency \n",
    "    # only 100 most frequently appearing bigrams are put into the list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible shortcomings to this approach: \n",
    "\n",
    "The bigram output actually contains a lot of bigrams that is composed of two context-independent stopwords of context-dependent stopwords. Such bigrams actually occur quite frequently in each document and could stop us from analysing the utility of bigrams in each text document. The output bigrams could be more meaningful if: \n",
    "\n",
    "* extract meaningful bigrams with collocations and different measures \n",
    "* remove bigrams composed of two context-independent stopwords\n",
    "\n",
    "\n",
    "## 5.1 Writing the 100 most frequent bigrams of each day and their frequencies to a file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleVocab = open('100bi.txt', 'w')\n",
    "\n",
    "for key, value in dailyBigrams.items(): \n",
    "    sampleVocab.write('{}:{}'.format(key, value))\n",
    "    sampleVocab.write('\\n')\n",
    "\n",
    "sampleVocab.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sparce Matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in vocabBigrams: \n",
    "    vocabUnigrams.add(i)\n",
    "    # created a unified set of corpus vocabularies \n",
    "    # this aims to enhance the efficiency when looping through the vocabs when creating a sparce matrix \n",
    "vocabs = vocabUnigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of tokens and their respective created date\n",
    "# the tokens in this dictionaries are the ones in corpusVocabs\n",
    "\n",
    "filteredToken=dict()\n",
    "# a dictionary for creating sparce matrix \n",
    "\n",
    "for date in tokens.keys(): \n",
    "    \n",
    "    # for the vocab of each day's tweets, filter out all the words that are not in the corpus vocabulary \n",
    "    filteredToken[date] = [stemmer.stem(w) \\\n",
    "           for w in tokens[date] \\\n",
    "           if stemmer.stem(w) in vocabs]\n",
    "\n",
    "    # filter all the words that are not in the corpus vocabularies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = open('countVec.txt', 'w')\n",
    "\n",
    "vocabDict = {vocab:index for index, vocab in enumerate(sorted(list(vocabs)))}\n",
    "# assign index ID to each vocabulary in corpus vocabs list\n",
    "\n",
    "for date, unigram in filteredToken.items(): \n",
    "    \n",
    "    matrix.write(date)\n",
    "    # the start of the line is the date (document name) of the tweets \n",
    "    \n",
    "    d_idx = [vocabDict[singleToken] for singleToken in unigram]\n",
    "    # create a list of index in respective of each token in the list of vocabs in the dictionary \n",
    "    \n",
    "    for k, v in fd(d_idx).items(): \n",
    "        matrix.write(',{}:{}'.format(k, v))\n",
    "        # write in the frequency of each index occurred in d_idx based on each document\n",
    "    matrix.write('\\n')\n",
    "\n",
    "matrix.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography  \n",
    "\n",
    "GALLAGHER, J. (2020, August 6). How to Sort a Dictionary by Value in Python. CAREER KARMA. https://careerkarma.com/blog/python-sort-a-dictionary-by-value/#:~:text=To%20sort%20a%20dictionary%20by%20value%20in%20Python%20you%20can,Dictionaries%20are%20unordered%20data%20structures.\n",
    "\n",
    "How to open multiple files in a directory. (2016, August 17). Stack Overflow. https://stackoverflow.com/questions/38991923/how-to-open-multiple-files-in-a-directory/38992988"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
